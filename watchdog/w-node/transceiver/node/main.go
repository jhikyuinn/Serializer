package main

import (
        "context"
        "crypto/rand"
        "crypto/rsa"
        "crypto/sha256"
        "crypto/sha512"
        "crypto/tls"
        "crypto/x509"
        "encoding/hex"
        "encoding/json"
        "encoding/pem"
        "strings"
        "flag"
        "fmt"
        "io"
        "log"
        "math/big"
        "net"
        "net/http"
        "regexp"
        "strconv"
        // "sync"
        "sync/atomic"
        "time"
        "os"
        "sort"

        lg "auditchain/ledger"
        pb "auditchain/msg"
        wp2p "auditchain/wp2p"

        "github.com/confluentinc/confluent-kafka-go/v2/kafka"
        "github.com/herumi/bls-eth-go-binary/bls"
        "github.com/prometheus/client_golang/prometheus/promhttp"
        "github.com/quic-go/quic-go"
        "google.golang.org/grpc"

        "go.mongodb.org/mongo-driver/bson"
        "go.mongodb.org/mongo-driver/mongo"
        "go.mongodb.org/mongo-driver/mongo/options"
)

// ======================= Structs =======================
type CONSENSUSNODE struct {
        address          string
        selfId           string
        roundIdx         uint32
        members          []*pb.Wnode
        isLeader         atomic.Bool
        leader           bool
        channelID        string
        done             chan bool
        pendingCommittee *pb.CommitteeMsg // ÏÉà Ïª§ÎØ∏Ìã∞ ÎåÄÍ∏∞Ïö©

        kafkaConsumer    *kafka.Consumer
        blockMsg         *pb.TransactionMsg
        memberMsg        *pb.MemberMsg
        auditMsg         *pb.AuditMsg
        cancel           context.CancelFunc
        isRunning        atomic.Bool
        isChildListening atomic.Bool
        publicKeyMap map[string]*bls.PublicKey
}

// ======================= Constants =======================
const (
        validatorIP       = "117.16.244.33"
        validatorGrpcPort = "16220"
        validatorNatPort  = "11730"
        gossipMsgPort     = ":4242"
        grpcPort          = ":5252"
        gossipListenPort  = ":6262"
        prometheusPort    = ":12345"
        abortTopic ="mychannel-abort"
        addrDB = "mongodb://127.0.0.1:27017"
)

type VRFNode struct {
        NodeID string
        PubKey []byte
        Seed   string
        Proof  []byte
        Value  *big.Int
}

var VRFGlobal *VRFNode

// ======================= Global Variables =======================
var (
startConsensusTime int64
nodename,_           = os.Hostname()
kafkaGroupID      = "GROUP"+nodename
seed string

// ======================= Kafka / Network =======================
validatorGrpcAddr string
validatorNatAddr  string
brokers           string
leaderChan        string

// ======================= BLS =======================
sec    bls.SecretKey // ÎÖ∏ÎìúÏùò BLS ÎπÑÎ∞ÄÌÇ§, VRFÏóêÎèÑ ÌôúÏö©
pub    *bls.PublicKey // ÎÖ∏ÎìúÏùò BLS Í≥µÍ∞úÌÇ§, VRFÏóêÎèÑ ÌôúÏö©
sigVec []bls.Sign // ÎÑ§Ìä∏ÏõåÌÅ¨Ïóê Ï∞∏Ïó¨ÌïòÎäî ÎÖ∏ÎìúÎì§Ïùò BLS ÏÑúÎ™Ö Î™®Ïùå
pubVec []bls.PublicKey // ÎÑ§Ìä∏ÏõåÌÅ¨Ïóê Ï∞∏Ïó¨ÌïòÎäî ÎÖ∏ÎìúÎì§Ïùò BLS Í≥µÍ∞úÌÇ§ Î™®Ïùå

        mongoclient *mongo.Client

// ======================= Context =======================
ctx    context.Context
cancel context.CancelFunc

// ======================= Consensus Status =======================
consensusStatusMap map[string]bool

)

// ======================= Main =======================
func main() {
        defaultValidatorAddr := flag.String("snode", "117.16.244.33", "Validator node IP address")
        kafkaProcessorAddr := flag.String("broker", "117.16.244.33", "Kafka broker IP address")
        libp2pAddr := flag.String("libp2p", "/ip4/117.16.244.33/tcp/4001/p2p/QmcpwcoEDeNfMjvGMNs5tyJvBh5SBAQXPTojFz8wDQT7KL", "Libp2p multiaddress")
        channel := flag.String("channel", "mychannel", "Channel name")
        consensusProtocol := flag.Bool("networktype", false, "true = MP2BTP, false = QUIC")

        flag.Parse()

        validatorGrpcAddr = *defaultValidatorAddr + ":16220"
        validatorNatAddr = *defaultValidatorAddr + ":11730"
        brokers = *kafkaProcessorAddr + ":9091," + *kafkaProcessorAddr + ":9092," + *kafkaProcessorAddr + ":9093"
        leaderChan = *channel

        var consensusNode CONSENSUSNODE
        consensusNode = CONSENSUSNODE{
                done:      make(chan bool, 1),
                blockMsg:  &pb.TransactionMsg{},
                auditMsg:  &pb.AuditMsg{},
                memberMsg: &pb.MemberMsg{},
        }

        

        go wp2p.Start(true, *libp2pAddr, 4010)
        time.Sleep(3 * time.Second)
        consensusNode.selfId = wp2p.Host
        fmt.Println("[HOST ID]", consensusNode.selfId)

        bls.Init(bls.BLS12_381)
        bls.SetETHmode(bls.EthModeDraft07)
        sec.SetByCSPRNG()
        pub = sec.GetPublicKey()

        http.Handle("/metrics", promhttp.Handler())
        // go monitoring()

        wp2p.JoinShard(leaderChan)

        consensusNode.start(*consensusProtocol)
}

func NewKafkaConsumer() *kafka.Consumer {
        c, err := kafka.NewConsumer(&kafka.ConfigMap{
                        "bootstrap.servers":       brokers,
                        "group.id":                kafkaGroupID,
                        "auto.offset.reset":       "earliest",
                        "enable.auto.commit":      false,
                        "fetch.min.bytes":         1,
                        "fetch.wait.max.ms":       10,
        })
        if err != nil {
                        log.Fatalf("Failed to create Kafka consumer: %v", err)
        }
        return c
}

func (w *CONSENSUSNODE) InitKafka() {
        w.kafkaConsumer = NewKafkaConsumer()

if err := w.kafkaConsumer.SubscribeTopics([]string{abortTopic}, nil); err != nil {
        log.Fatalf("‚ùå Failed to subscribe Kafka topic %s: %v", abortTopic, err)
}

ctx, cancel := context.WithCancel(context.Background())
w.cancel = cancel

go w.KafkaListener(ctx, abortTopic)
}

func (w *CONSENSUSNODE) setLeaderChan(isLeader bool, channel string) {
        if w.channelID != channel {
                        w.channelID = channel
        }
        w.isLeader.Store(isLeader)
}

func (w *CONSENSUSNODE) setDone(b bool) {
        w.done <- b
}

func (w *CONSENSUSNODE) start(consensusProtocol bool) {
        w.getOutboundIP()
        w.isLeader.Store(false)
        w.done = make(chan bool, 1)
        w.isRunning.Store(false)

        consensusStatusMap = make(map[string]bool)

        go w.InitKafka()

        go w.CommitteeListening()
        go w.ConsensusListening()
        go w.BlockListening()

        w.publicKeyMap = make(map[string]*bls.PublicKey)

        // [GRPC: Membership Message]
        w.Reporting()
        time.Sleep(3 * time.Second)
        ReportExternal(w.selfId)
        w.ListenForCommitteeMsgs()
}

func (w *CONSENSUSNODE) ListenForCommitteeMsgs() {
        for {
            msg := <-committeeMsgChannel // Ïñ¥Îñ§ Î∞©ÏãùÏúºÎ°úÎì† Î©îÏãúÏßÄÎ•º ÏàòÏã†ÌñàÎã§Í≥† Í∞ÄÏ†ï
            log.Println(msg)
        }
}

var committeeMsgChannel = make(chan *pb.CommitteeMsg, 10)

// ======================= Committee Listening =======================
func (w *CONSENSUSNODE) CommitteeListening() {
        listener, err := net.Listen("tcp", w.address+gossipListenPort)
        if err != nil {
                        log.Fatalf("Failed to listen on %s: %v", w.address+gossipListenPort, err)
        }
        defer listener.Close()

        log.Println("üü¢ Committee Listening on", w.address+gossipListenPort)

        for {
                conn, err := listener.Accept()
                if err != nil {
                                log.Println("‚ö†Ô∏è Accept error:", err)
                                continue
                }
                go w.CommConnHandler(conn)
        }
}

func (w *CONSENSUSNODE) CommConnHandler(conn net.Conn) {
        defer conn.Close()

        recvBuf := make([]byte, 20000)
        n, err := conn.Read(recvBuf)
        if err != nil {
                if err != io.EOF {
                log.Printf("‚ùå Failed to read data: %v", err)
                }
                return
        }

        recvMsg := &pb.CommitteeMsg{}
        if err := json.Unmarshal(recvBuf[:n], recvMsg); err != nil {
                log.Printf("‚ùå Failed to unmarshal CommitteeMsg: %v", err)
                return
        }

        if w.isRunning.Load() {
                w.pendingCommittee = recvMsg
                log.Printf("‚ö†Ô∏è Still processing previous consensus round, storing new CommitteeMsg (round %d) as pending", recvMsg.RoundNum)
                return
        }

        w.applyCommittee(recvMsg)
}

// // Check if this peer is the leader; if so, it should receive messages from Kafka.
// // BlockListening listens for block insertion.
func (w *CONSENSUSNODE) BlockListening() {
        w.getOutboundIP()
        listener, err := net.Listen("tcp", w.address+grpcPort)
        if err != nil {
                log.Fatalf("‚ùå Failed to listen on %s: %v", w.address+grpcPort, err)
        }
        defer listener.Close()

        for {
                conn, err := listener.Accept()
                if err != nil {
                        log.Println("‚ö†Ô∏è Accept error:", err)
                        continue
                }
                go w.BloConnHandler(conn)
        }
}

func (w *CONSENSUSNODE) BloConnHandler(conn net.Conn) {
        defer conn.Close()

        recvBuf := make([]byte, 8192)
        n, err := conn.Read(recvBuf)
        if err != nil {
                if err == io.EOF {
                        log.Printf("üîå Connection closed by client: %v", conn.RemoteAddr())
                } else {
                        log.Printf("‚ùå Failed to read data: %v", err)
                }
                return
        }

        MsgRecv := &pb.GossipMsg{}
        if err := json.Unmarshal(recvBuf[:n], MsgRecv); err != nil {
                log.Printf("‚ùå Failed to unmarshal GossipMsg: %v", err)
                return
        }

        fmt.Println("üì¶ BLOCKINSERT: Committing block to ledger")
        go lg.BlkInsert(MsgRecv.Rndblk)
        w.Reporting()
}
    

type VRFCandidate struct {
        Node *pb.Wnode
        VRF  *big.Int
}

func (w *CONSENSUSNODE) applyCommittee(msg *pb.CommitteeMsg) {
	w.done = make(chan bool, 1)
	w.roundIdx = msg.RoundNum

	for _, shard := range msg.Shards {
		// Ìï¥Îãπ shardÏóê ÎÇ¥Í∞Ä Ìè¨Ìï®ÎêòÏñ¥ ÏûàÎäîÏßÄ ÌôïÏù∏
		containsSelf := false
		for _, mem := range shard.Member {
			if mem.NodeID == w.selfId {
				containsSelf = true
				break
			}
		}
		if !containsSelf {
			continue
		}

		// === (1) VRF Í≤ÄÏ¶ù Î∞è ÌõÑÎ≥¥ Î¶¨Ïä§Ìä∏ ÏÉùÏÑ± ===
		var candidates []VRFCandidate
		for _, mem := range shard.Member {
			var pubKey bls.PublicKey
			if err := pubKey.Deserialize(mem.Publickey); err != nil {
				fmt.Printf("‚ùå Node %s PublicKey Ïó≠ÏßÅÎ†¨Ìôî Ïã§Ìå®\n", mem.NodeID)
				return
			}
                        w.publicKeyMap[mem.NodeID] = &pubKey

			vrfVal, ok := VerifyVRF(mem.Seed, mem.Proof, pubKey)
			if !ok {
				fmt.Printf("‚ùå Node %s VRF Í≤ÄÏ¶ù Ïã§Ìå®\n", mem.NodeID)
				return
			}
			fmt.Printf("‚úÖ Node %s VRF Í≤ÄÏ¶ù ÏÑ±Í≥µ\n", mem.NodeID)

			candidates = append(candidates, VRFCandidate{
				Node: mem,
				VRF:  vrfVal,
			})
		}

		// === (2) VRF ÏàúÏúºÎ°ú Ï†ïÎ†¨ ===
		sort.Slice(candidates, func(i, j int) bool {
			return candidates[i].VRF.Cmp(candidates[j].VRF) < 0
		})

		// === (3) ÏúÑÏ°∞ Ïó¨Î∂Ä ÌôïÏù∏ (ÏàúÏÑú Î∞è Î¶¨ÎçîID) ===
		for i := range shard.Member {
			if shard.Member[i].NodeID != candidates[i].Node.NodeID {
				fmt.Println("‚ùå Committee Î©§Î≤Ñ ÏàúÏÑú Î∂àÏùºÏπò ‚Üí Î©îÏãúÏßÄ ÏúÑÏ°∞ Í∞ÄÎä•ÏÑ±")
				return
			}
		}
		if shard.LeaderID != candidates[0].Node.NodeID {
			fmt.Println("‚ùå Leader Î∂àÏùºÏπò ‚Üí Î©îÏãúÏßÄ ÏúÑÏ°∞ Í∞ÄÎä•ÏÑ±")
			return
		}

		// === (4) Ìï©Ïùò ÏÉÅÌÉú Ï†ÅÏö© ===
		log.Printf("üîπ CommitteeMsg Round: %d Ï†ÅÏö©Îê®", msg.RoundNum)
		w.members = shard.Member
		w.leader = (shard.LeaderID == w.selfId)
		w.channelID = leaderChan

		if w.leader {
			fmt.Println("üéâ Leader consensus node")
			go func() {
				defer w.setDone(false)
				defer w.isRunning.Store(false)
				w.StartListener(w.channelID + "-abort")
			}()
		} else {
			fmt.Println("üß© Follower consensus node")
			w.StopListener()
		}
		return
	}

	fmt.Println("CommitteeMsgÏóê Ìï¥Îãπ ÎÖ∏ÎìúÍ∞Ä Ìè¨Ìï®ÎêòÏñ¥ ÏûàÏßÄ ÏïäÏùå")
}

func VerifyVRF(seed string, proof []byte, pubKey bls.PublicKey) (*big.Int, bool) {
        var sig bls.Sign
        if err := sig.Deserialize(proof); err != nil {
            return nil, false
        }
    
        seedHash := sha256.Sum256([]byte(seed))
        if !sig.FastAggregateVerify([]bls.PublicKey{pubKey}, seedHash[:]) {
            return nil, false
        }
    
        sigHash := sha256.Sum256(proof)
        vrfValue := new(big.Int).SetBytes(sigHash[:])
    
        return vrfValue, true
    }
    
    func VRFHash(nodeID, seed string) *big.Int { 
            data := []byte(nodeID + seed) 
            hash := sha256.Sum256(data) 
            return new(big.Int).SetBytes(hash[:]) 
    }


func (w *CONSENSUSNODE) StartListener(topic string) {
        if w.isRunning.Load() {
                return
        }

        ctx, cancel := context.WithCancel(context.Background())
        w.cancel = cancel
        w.isRunning.Store(true)

        go func() {
                defer w.isRunning.Store(false)
                w.KafkaListener(ctx, topic)
        }()
}

func (w *CONSENSUSNODE) StopListener() {
        if w.cancel != nil {
                w.cancel()
                fmt.Println("Listener Ï§ëÎã® ÏöîÏ≤≠")
        }
}


// ======================= Kafka Listener =======================
type AbortPayload struct {
Timestamp int64                `json:"timestamp"`
Data      []*pb.TransactionMsg `json:"data"`
}

func (w *CONSENSUSNODE) KafkaListener(ctx context.Context, topic string) {
        re := regexp.MustCompile(`User(\d+)`)

        for {
        select {
        case <-ctx.Done():
                fmt.Println("KafkaListener stopped")
                return
        default:
                msg, err := w.kafkaConsumer.ReadMessage(-1)
                if err != nil {
                        log.Printf("Kafka read error: %v", err)
                        continue
                }

                _, err = w.kafkaConsumer.CommitMessage(msg)
                if err != nil {
                        log.Printf("Commit failed: %v", err)
                }

                if !w.leader {
                        continue
                }

                // JSON Ïñ∏ÎßàÏÉ¨
                var abortPayload AbortPayload
                err = json.Unmarshal(msg.Value, &abortPayload)
                if err != nil {
                        log.Printf("Failed to unmarshal Kafka message: %v", err)
                        continue
                }

                userCount := make(map[string]int32)
                for _, data := range abortPayload.Data {
                        match := re.FindStringSubmatch(data.String())
                        if len(match) > 1 {
                                idx := idxToInt(match[1])
                                userKey := "User" + strconv.Itoa(idx)
                                userCount[userKey]++
                        }
                }

                startConsensusTime = abortPayload.Timestamp
                w.auditMsg = &pb.AuditMsg{}
                w.bftConsensus(userCount)
                }
        }
}

func GetMongoClient() {
        ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
        defer cancel()
        mongoclient, _ = mongo.Connect(ctx, options.Client().ApplyURI(addrDB))
}

func GetPrevBlockHash() (string, error) {
        GetMongoClient()
        collection := mongoclient.Database("ledger").Collection("blk")

        var lastBlock struct {
                CurHash string `bson:"CurHash"`
        }

        opts := options.FindOne().SetSort(bson.D{{Key: "_id", Value: -1}})

        err := collection.FindOne(context.Background(), bson.D{}, opts).Decode(&lastBlock)
        if err != nil {
                if err == mongo.ErrNoDocuments {
                        // Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏùÑ Í≤ΩÏö∞ Í∏∞Î≥∏Í∞í Î∞òÌôò
                        defaultHash := strings.Repeat("0", 64) // 64ÏûêÎ¶¨ 0 (SHA-256 Í∏∞Î≥∏ Í∏∏Ïù¥)
                        return defaultHash, nil
                }
                return "", err // Îã§Î•∏ ÏóêÎü¨Ïùº Í≤ΩÏö∞ Í∑∏ÎåÄÎ°ú Î∞òÌôò
        }

        return lastBlock.CurHash, nil
}
func (w *CONSENSUSNODE) bftConsensus(userCount map[string]int32) {

        PrevHash, _ := GetPrevBlockHash()

        w.auditMsg = &pb.AuditMsg{
                BlkNum:              w.roundIdx,
                LeaderID:            w.selfId,
                PrevHash:	     PrevHash,
                PhaseNum:            pb.AuditMsg_PREPARE,
                MerkleRootHash:      "abcdefghijklmnopqrstuvwxyz",
                Aborttransactionnum: userCount,
                HonestAuditors: []*pb.HonestAuditor{
                        {Id: w.selfId},
                },
        }

        hashBytes := lg.BlockHashCalculator(w.auditMsg)
        hash := sha512.Sum512(hashBytes)
        w.auditMsg.CurHash = hex.EncodeToString(hash[:63])

        signing := sec.SignByte([]byte(w.auditMsg.CurHash))
        w.auditMsg.Signature = signing.Serialize()

        // ==== PREPARE & COMMIT Broadcasting ====
        fmt.Println("üöÄ Broadcasting PREPARE")
        chPrepare := make(chan *pb.AuditMsg, len(w.members))
        for _, each := range w.members {
                        go w.Submit(chPrepare, each.Addr, pb.AuditMsg_PREPARE, pb.AuditMsg_AGGREGATED_PREPARE)
        }

        quorum := len(w.members)
        w.waitForVotes(chPrepare, quorum, pb.AuditMsg_AGGREGATED_PREPARE)

        fmt.Println("üöÄ Broadcasting COMMIT")
        chCommit := make(chan *pb.AuditMsg, len(w.members))
        for _, each := range w.members {
                        go w.Submit(chCommit, each.Addr, pb.AuditMsg_COMMIT, pb.AuditMsg_AGGREGATED_COMMIT)
        }
        w.waitForVotes(chCommit, quorum, pb.AuditMsg_AGGREGATED_COMMIT)

        fmt.Println("üì¶ Disseminating block to peers")
        gossipMsg := &pb.GossipMsg{
                        Type:   2,
                        Rndblk: w.auditMsg,
        }
        wp2p.WDNMessage(wp2p.Wctx, wp2p.Shard[0], gossipMsg)
        elapsed := time.Now().UnixMilli() - startConsensusTime
        fmt.Println("consensus elapsed time (milliseconds):", elapsed)
}

/*I'm a leader, send a message to a w-node*/
func (w *CONSENSUSNODE) Submit(ch chan<- *pb.AuditMsg, ip string, currentPhase pb.AuditMsg_Phases, targetPhase pb.AuditMsg_Phases) {
        tlsConf := &tls.Config{
                InsecureSkipVerify: true,
                NextProtos:         []string{"quic-echo-example"},
        }
        session, err := quic.DialAddr(context.Background(), ip+gossipMsgPort, tlsConf, nil)
        if err != nil {
                log.Printf("‚ùå Failed to connect to %s: %v", ip+gossipMsgPort, err)
                ch <- nil
                return
        }

        defer session.CloseWithError(0, "")

        stream, err := session.OpenStreamSync(context.Background())
        if err != nil {
                log.Printf("‚ùå Failed to open stream: %v", err)
                ch <- nil
                return
        }

        w.auditMsg.PhaseNum = currentPhase

        sndBuf, err := json.Marshal(w.auditMsg)
        if err != nil {
                log.Printf("‚ùå Marshal error: %v", err)
                ch <- nil
                return
        }
        _, err = stream.Write(sndBuf)
        if err != nil {
                log.Printf("‚ùå Write error: %v", err)
                ch <- nil
                return
        }

        rcvBuf := make([]byte, 8192)
        n, err := stream.Read(rcvBuf)
        if err != nil || n == 0 {
                ch <- nil
                return
        }

        msg := &pb.AuditMsg{}
        if err := json.Unmarshal(rcvBuf[:n], msg); err != nil {
                log.Println("‚ùå JSON unmarshal error:", err)
                ch <- nil
                return
        }

        if msg.PhaseNum == targetPhase {
                ch <- msg
        } else {
                ch <- nil
        }
}

/*Consensus three-phases: Announce(Completed State), Prepare, Commit] I'm not a leader*/
func (w *CONSENSUSNODE) ConsensusListening() {
        listener, err := quic.ListenAddr(w.address+gossipMsgPort, generateTLSConfig(), nil)
        if err != nil {
                log.Fatalf("‚ùå QUIC Listen error: %v", err)
        }

        fmt.Println("üü£ Consensus data Listening on", w.address+gossipMsgPort)

        for {
                sess, err := listener.Accept(context.Background())
                if err != nil {
                log.Println("‚ùå Session accept error:", err)
                continue
                }

                stream, err := sess.AcceptStream(context.Background()) 
                        if err != nil { 
                                panic(err) 
                        } 
                        go w.StreamHandler(stream)
        }
}

func (w *CONSENSUSNODE) StreamHandler(stream quic.Stream) {

        buf := make([]byte, 8192) 

        for {
                n, err := stream.Read(buf)
                if err != nil {
                if err.Error() != "EOF" {
                        log.Println("‚ùå Stream read error:", err)
                }
                return
                }

                if n == 0 {
                continue
                }

                msg := &pb.AuditMsg{}
                if err := json.Unmarshal(buf[:n], msg); err != nil {
                log.Println("‚ùå JSON unmarshal error:", err)
                continue
                }

                // log.Println("DATA FROM LEADER:", msg)

                switch msg.PhaseNum {
                case pb.AuditMsg_PREPARE:
                        w.sendResponse(pb.AuditMsg_AGGREGATED_PREPARE,msg,stream)
                        return
                case pb.AuditMsg_COMMIT:
                        if w.verifying(msg) {
                                w.sendResponse(pb.AuditMsg_AGGREGATED_COMMIT,msg,stream)
                                return
                        }
                        default:
                        continue
                }
        }
}

func (w *CONSENSUSNODE) updateAuditFields(cMsg *pb.AuditMsg) {

        alreadyExists := false
        for _, id := range cMsg.HonestAuditors {
                if id.Id == w.selfId {
                        alreadyExists = true
                        break
                }
        }
        if !alreadyExists {
                w.auditMsg.HonestAuditors = append(cMsg.HonestAuditors, &pb.HonestAuditor{Id: w.selfId})
        } else {
                w.auditMsg.HonestAuditors = cMsg.HonestAuditors
        }
}

func (w *CONSENSUSNODE) sendResponse(targetPhase pb.AuditMsg_Phases,cMsg *pb.AuditMsg,stream quic.Stream) {

        copied := *cMsg

        hash := []byte(copied.CurHash)
        signing := sec.SignByte(hash)

        copied.Signature = signing.Serialize()
        copied.PhaseNum = targetPhase

        w.auditMsg = &copied
        w.updateAuditFields(&copied)

        sndBuf, err := json.Marshal(w.auditMsg)
        if err != nil {
                panic(err)
        }
        if _, err = stream.Write(sndBuf); err != nil {
                panic(err)
        }
}

func (w *CONSENSUSNODE) Multisinging(cMsgs *pb.AuditMsg, sigVec []bls.Sign) (bool, int) {
        switch cMsgs.PhaseNum {
        case pb.AuditMsg_AGGREGATED_PREPARE:
                return w.aggregateAndPrepare(cMsgs, sigVec), len(w.auditMsg.HonestAuditors)

        case pb.AuditMsg_AGGREGATED_COMMIT:
                return w.aggregateAndCommit(cMsgs, sigVec), len(w.auditMsg.HonestAuditors)

        default:
                return false, 0
        }
}

func (w *CONSENSUSNODE) aggregateAndPrepare(cMsgs *pb.AuditMsg, sigVec []bls.Sign) bool {
        return w.aggregateAndSet(cMsgs, sigVec, pb.AuditMsg_COMMIT)
}

func (w *CONSENSUSNODE) aggregateAndCommit(cMsgs *pb.AuditMsg, sigVec []bls.Sign) bool {
        return w.aggregateAndSet(cMsgs, sigVec, pb.AuditMsg_AGGREGATED_COMMIT)
}

func (w *CONSENSUSNODE) aggregateAndSet(cMsgs *pb.AuditMsg, sigVec []bls.Sign, phase pb.AuditMsg_Phases) bool {
        var aggeSign bls.Sign
        aggeSign.Aggregate(sigVec)
        byteSig := aggeSign.Serialize()

        w.auditMsg.BlkNum = cMsgs.BlkNum
        w.auditMsg.PrevHash = cMsgs.PrevHash
        w.auditMsg.CurHash = cMsgs.CurHash
        w.auditMsg.Signature = byteSig
        w.auditMsg.PhaseNum = phase
        return true
}

func (w *CONSENSUSNODE) verifying(cMsg *pb.AuditMsg) bool {
        pubVec = pubVec[:0]
        seenKeys := make(map[string]bool)

        for _, honestID := range cMsg.HonestAuditors {
                if pk := w.getPublicKeyByNodeID(honestID.Id); pk != nil {
                        keyBytes := pk.Serialize()
                        keyStr := string(keyBytes) // serializeÎêú Î∞îÏù¥Ìä∏Î•º Î¨∏ÏûêÏó¥Î°ú

                        if !seenKeys[keyStr] {
                        pubVec = append(pubVec, *pk)
                        seenKeys[keyStr] = true
                        } else {
                        fmt.Printf("‚ö†Ô∏è Duplicate public key ignored for node: %s\n", honestID.Id)
                        }
                } else {
                        fmt.Printf("Missing public key for node: %s\n", honestID.Id)
                }
        }
        

        var decSign bls.Sign
        if err := decSign.Deserialize(cMsg.Signature); err != nil {
                fmt.Println("Signature deserialization error:", err)
                return false
        }

        switch cMsg.PhaseNum {
        case pb.AuditMsg_COMMIT:
                if decSign.FastAggregateVerify(pubVec, []byte(cMsg.CurHash)) {
                        fmt.Println("‚úÖ AGGREGATED_COMMIT: Verification SUCCESS")
                        return true
                }  
                fmt.Println("‚ùå AGGREGATED_COMMIT: Verification ERROR")
        }
        return false
}

func (w *CONSENSUSNODE) getPublicKeyByNodeID(id string) *bls.PublicKey {
        if pk, ok := w.publicKeyMap[id]; ok {
		return pk
	}
	fmt.Printf("üîç Public key for node %s not found in publicKeyMap\n", id)
	return nil
}

func (w *CONSENSUSNODE) getOutboundIP() string {
        conn, err := net.Dial("udp", "8.8.8.8:80")
        if err != nil {
                panic(err)
        }
        defer conn.Close()
        localAddr := conn.LocalAddr().(*net.UDPAddr)
        w.address=localAddr.IP.String()

        return localAddr.IP.String()
}

func GenerateVRF(nodeID string, seed string) {
        seed = strconv.FormatInt(time.Now().UnixMilli(), 10)
        nodeinit := &VRFNode{
                NodeID: nodeID,
                PubKey: pub.Serialize(),
        }

        hash := sha256.Sum256([]byte(seed))
        sign := sec.SignByte(hash[:]) 
        nodeinit.Proof = sign.Serialize()
        nodeinit.Seed = seed
        vrfHash := sha256.Sum256(sign.Serialize())
        nodeinit.Value = new(big.Int).SetBytes(vrfHash[:])
        VRFGlobal = nodeinit

}

/*GRPC SERVICE Membershhip Exchanging Procedure*/
func (w *CONSENSUSNODE) Reporting() {
        conn, err := grpc.Dial(validatorGrpcAddr, grpc.WithInsecure(), grpc.WithBlock())
        if err != nil {
                log.Fatalf("Failed to connect to validator gRPC server: %v", err)
        }
        defer conn.Close()

        c := pb.NewMembershipServiceClient(conn)
        GenerateVRF(w.address, seed)
        req := createSelfMembership(w.selfId, w.address,"11730", VRFGlobal)

        w.memberMsg, err = c.GetMembership(context.Background(), req)
        if err != nil {
                log.Fatalf("Failed to get membership: %v", err)
        }
}

func createSelfMembership(id string, addr string, port string, nodeinfo *VRFNode) *pb.MemberMsg {

        node := &pb.Node{
                NodeID:    id,
                Addr:      addr,
                Port:      port,
                Publickey: nodeinfo.PubKey,
                Seed:      nodeinfo.Seed,
                Proof:     nodeinfo.Proof,
                Value:     nodeinfo.Value.Bytes(),
        }
        return &pb.MemberMsg{Nodes: []*pb.Node{node}}
}

func MerkleHash(s []uint64) string {
        return strconv.FormatUint(s[0], 16)     
}

func ReportExternal(id string) {
        conn, err := net.Dial("tcp", validatorNatAddr)
        if err != nil {
                fmt.Println("[NAT] Failed to dial:", err)
                return
        }
        defer conn.Close()

        sndBuf, err := json.Marshal(id)
        if err != nil {
                fmt.Println("[NAT] Failed to marshal ID:", err)
                return
        }

        if _, err := conn.Write(sndBuf); err != nil {
                fmt.Println("[NAT] Failed to write data:", err)
        }
}

func generateTLSConfig() *tls.Config {
        key, err := rsa.GenerateKey(rand.Reader, 1024)
        if err != nil {
                panic(fmt.Errorf("failed to generate RSA key: %w", err))
        }

        template := x509.Certificate{SerialNumber: big.NewInt(1)}
        certDER, err := x509.CreateCertificate(rand.Reader, &template, &template, &key.PublicKey, key)
        if err != nil {
                panic(fmt.Errorf("failed to create certificate: %w", err))
        }

        keyPEM := pem.EncodeToMemory(&pem.Block{Type: "RSA PRIVATE KEY", Bytes: x509.MarshalPKCS1PrivateKey(key)})
        certPEM := pem.EncodeToMemory(&pem.Block{Type: "CERTIFICATE", Bytes: certDER})

        tlsCert, err := tls.X509KeyPair(certPEM, keyPEM)
        if err != nil {
                panic(fmt.Errorf("failed to load TLS key pair: %w", err))
        }

        return &tls.Config{
                Certificates: []tls.Certificate{tlsCert},
                NextProtos:   []string{"quic-echo-example"},
        }
}

func idxToInt(s string) int {
        var num int
        fmt.Sscanf(s, "%d", &num)
        return num
}

func setConsensusStart(id string) {
        consensusStatusMap[id] = false  
}

func setConsensusDone(id string) {
        consensusStatusMap[id] = true
}

func (w *CONSENSUSNODE) waitForVotes(ch chan *pb.AuditMsg, quorum int, targetPhase pb.AuditMsg_Phases) bool {
        votes := 0
        var sigVec []bls.Sign

        honestAuditorsSet := make(map[string]*pb.HonestAuditor)

        for {
                msg := <-ch // blocking read
                if msg == nil {
                        continue
                }

                sig := bls.Sign{}
                if err := sig.Deserialize(msg.Signature); err != nil {
                        log.Println("‚ùå Signature deserialize error:", err)
                        continue
                }

                sigVec = append(sigVec, sig)
                votes++
                fmt.Printf("üó≥Ô∏è Votes: %d/%d\n", votes, quorum)

                for _, auditor := range msg.HonestAuditors {
                        if auditor != nil {
                                honestAuditorsSet[auditor.Id] = auditor
                        }
                }

                // map ‚Üí sliceÎ°ú Î≥ÄÌôò
                w.auditMsg.HonestAuditors = make([]*pb.HonestAuditor, 0, len(honestAuditorsSet))
                for _, auditor := range honestAuditorsSet {
                        w.auditMsg.HonestAuditors = append(w.auditMsg.HonestAuditors, auditor)
                }

                if votes >= quorum {
                        fmt.Printf("‚úÖ Phase %v Aggregation Complete\n", targetPhase)
                        w.Multisinging(msg, sigVec)
                        return true
                }
        }
}

// ======================= Monitoring =======================
// func monitoring() {
//   counter := prometheus.NewCounter(prometheus.CounterOpts{Namespace: "WDN", Name: "counter_total"})
//   gauge := prometheus.NewGauge(prometheus.GaugeOpts{Namespace: "WDN", Name: "gauge_value"})
//   histogram := prometheus.NewHistogram(prometheus.HistogramOpts{
//           Namespace: "WDN", Name: "histogram_value", Buckets: prometheus.LinearBuckets(0, 5, 10),
//   })

//   prometheus.MustRegister(counter, gauge, histogram)

//   go func() {
//           for {
//                   counter.Add(rand.Float64() * 5)
//                   gauge.Add(rand.Float64()*15 - 5)
//                   histogram.Observe(rand.Float64() * 10)
//                   time.Sleep(2 * time.Second)
//           }
//   }()
//   fmt.Println(http.ListenAndServe(prometheusPort, nil))
// }